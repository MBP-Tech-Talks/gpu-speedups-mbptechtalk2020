{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5_intro_pycuda.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/geoffwoollard/gpu-speedups-mbptechtalk2020/blob/master/5_intro_pycuda.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79QTakANiS18",
        "colab_type": "text"
      },
      "source": [
        "# MBP Tech Talk 2020 :: Intro to PyCUDA\n",
        "PyCUDA is a way to get a CUDA C kernel into python. If you come across an existing CUDA C file and don't want to rewrite it python to make a numba cuda kernel, pycuda is one option.\n",
        "\n",
        "You might come across, the `skcuda` library, which is built on top of `pycuda`. It is developed independently and includes [high level routines](https://scikit-cuda.readthedocs.io/en/latest/reference.html#high-level-routines) like linear algebra (matrix operations, svd, PCA, eigencevtors and values, etc).\n",
        "\n",
        "The following notebook is from the [pycuda tutorial](https://documen.tician.de/pycuda/index.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvV7bouws2lc",
        "colab_type": "text"
      },
      "source": [
        "`pycuda` is not installed on google colab, but we can get it with `pip`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imIBRAKLiiVh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install pycuda"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dThreCgwiPct",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pycuda.driver as cuda\n",
        "import pycuda.autoinit\n",
        "from pycuda.compiler import SourceModule\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHSLC2p5i0Eu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# made random 4x4 array and copy over to GPU\n",
        "a = np.random.randn(4,4)\n",
        "a = a.astype(np.float32) #nvidia single precision\n",
        "a_gpu = cuda.mem_alloc(a.nbytes) # allocate memory on device\n",
        "cuda.memcpy_htod(a_gpu,a) # transfer data to GPU"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IMFCJ5Kjara",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The string below is in CUDA C. In large coding projects people separate this into a `.cu` file.\n",
        "mod = SourceModule(\"\"\"\n",
        "  __global__ void doublify(float *a)\n",
        "  {\n",
        "    int idx = threadIdx.x + threadIdx.y*4; // comments allowed\n",
        "    a[idx] *= 2;\n",
        "  }\n",
        "  \"\"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpzU4Y1Fvxn2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# SourceModule can catch compiler errors, and gives error messages for troubleshooting \n",
        "SourceModule(\"\"\"\n",
        "  __global__ void doublify(float *a)\n",
        "  {\n",
        "    int idx = threadIdx.x + threadIdx.y*4 // missing ;\n",
        "    a[idx] *= 2;\n",
        "  }\n",
        "  \"\"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VjFmACcj0EX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "func = mod.get_function(\"doublify\")\n",
        "func(a_gpu, block=(4,4,1), grid=(1,1,1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H04abHktkBdu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a_doubled = np.empty_like(a)\n",
        "cuda.memcpy_dtoh(a_doubled,a_gpu)\n",
        "print(a_doubled)\n",
        "print(2*a)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwufiUevw8uq",
        "colab_type": "text"
      },
      "source": [
        "# `pycuda.gpuarray.GPUArray`\n",
        "`pycuda.gpuarray.GPUArray` abstracts away much of the data trasfers and memory allocation mentioned above. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0wH9EaAkzDF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pycuda.gpuarray as gpuarray\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9DLTMuik3Wz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a_gpu = gpuarray.to_gpu(np.random.randn(4,4).astype(np.float32))\n",
        "a_doubled = (2*a_gpu).get()\n",
        "print(a_doubled)\n",
        "print(a_gpu)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}