{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"},"colab":{"name":"2 - Memory Management.ipynb","provenance":[{"file_id":"https://github.com/geoffwoollard/gpu-speedups-mbptechtalk2020/blob/master/3%20-%20Memory%20Management.ipynb","timestamp":1580070532396}],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"jWwQTt6BioUq","colab_type":"text"},"source":["# MBP Tech Talk 2020 :: Memory Management\n","\n","## Managing GPU Memory\n","\n","During the benchmarking in the previous notebook, we used NumPy arrays on the CPU as inputs and outputs.  If you want to reduce the impact of host-to-device/device-to-host bandwidth, it is best to copy data to the GPU explicitly and leave it there to amortize the cost over multiple function calls.  In addition, allocating device memory can be relatively slow, so allocating GPU arrays once and refilling them with data from the host can also be a performance improvement.\n","\n","Let's create our example addition ufunc again:"]},{"cell_type":"code","metadata":{"id":"Fn3jaQBZioUu","colab_type":"code","colab":{}},"source":["from numba import vectorize\n","import numpy as np\n","\n","@vectorize(['float32(float32, float32)'], target='cuda')\n","def add_ufunc(x, y):\n","    return x + y"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ewKn0K44ioUy","colab_type":"code","colab":{}},"source":["n = 100000\n","x = np.arange(n).astype(np.float32)\n","y = 2 * x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ruoYI6vZioU1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"0016663b-d676-4b75-80f3-39eee9783120","executionInfo":{"status":"ok","timestamp":1580070620540,"user_tz":300,"elapsed":780,"user":{"displayName":"Geoffrey Woollard","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDjNy-VYGrpEYVchU7TMBqcYGYexvL_pBIe7j5R=s64","userId":"09068754665017449583"}}},"source":["%timeit add_ufunc(x, y)  # Baseline performance with host arrays"],"execution_count":3,"outputs":[{"output_type":"stream","text":["The slowest run took 310.38 times longer than the fastest. This could mean that an intermediate result is being cached.\n","1 loop, best of 3: 1.46 ms per loop\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CtcEpICkioU5","colab_type":"text"},"source":["There are two ways that we can create GPU arrays to pass to Numba.  Numba defines its own GPU array object (not as fully-featured as CuPy, but may be useful if you don't need the rest of CuPy for your application).  The `numba.cuda` module includes a function that will copy host data to the GPU and return a CUDA device array:"]},{"cell_type":"code","metadata":{"id":"2-RHjwdEioU6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"047ad28c-9bbd-45fa-9f49-4d19b030c496","executionInfo":{"status":"ok","timestamp":1580070622700,"user_tz":300,"elapsed":210,"user":{"displayName":"Geoffrey Woollard","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDjNy-VYGrpEYVchU7TMBqcYGYexvL_pBIe7j5R=s64","userId":"09068754665017449583"}}},"source":["from numba import cuda\n","\n","x_device = cuda.to_device(x)\n","y_device = cuda.to_device(y)\n","\n","print(x_device)\n","print(x_device.shape)\n","print(x_device.dtype)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["<numba.cuda.cudadrv.devicearray.DeviceNDArray object at 0x7f225c28e278>\n","(100000,)\n","float32\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_DBBMYnYioU9","colab_type":"text"},"source":["Device arrays can be passed to Numba's compiled CUDA functions just like NumPy arrays, but without the copy overhead:"]},{"cell_type":"code","metadata":{"id":"G0TgfG_XioU-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"6ff8f178-6741-4df1-8b22-825ea293349b","executionInfo":{"status":"ok","timestamp":1580070629934,"user_tz":300,"elapsed":5140,"user":{"displayName":"Geoffrey Woollard","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDjNy-VYGrpEYVchU7TMBqcYGYexvL_pBIe7j5R=s64","userId":"09068754665017449583"}}},"source":["%timeit add_ufunc(x_device, y_device)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["1000 loops, best of 3: 1.16 ms per loop\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"FxZ8YkdQioVC","colab_type":"text"},"source":["That's a big performance improvement already, but we are still allocating a device array for the output of the ufunc and copying it back to the host.  We can create the output buffer with the `numba.cuda.device_array()` function:"]},{"cell_type":"code","metadata":{"id":"c0NhzOIBioVD","colab_type":"code","colab":{}},"source":["out_device = cuda.device_array(shape=(n,), dtype=np.float32)  # does not initialize the contents, like np.empty()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"Paa8fJihioVG","colab_type":"text"},"source":["And then we can use a special `out` keyword argument to the ufunc to specify the output buffer:"]},{"cell_type":"code","metadata":{"id":"Ad5S9EFnioVK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"a468edac-0d03-48cf-a4e4-ebf6e3a4ff74","executionInfo":{"status":"ok","timestamp":1580070663357,"user_tz":300,"elapsed":5708,"user":{"displayName":"Geoffrey Woollard","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDjNy-VYGrpEYVchU7TMBqcYGYexvL_pBIe7j5R=s64","userId":"09068754665017449583"}}},"source":["%timeit add_ufunc(x_device, y_device, out=out_device)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["The slowest run took 5.38 times longer than the fastest. This could mean that an intermediate result is being cached.\n","1000 loops, best of 3: 1.28 ms per loop\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"z51-ROi2ioVN","colab_type":"text"},"source":["Now that we have removed the device allocation and copy steps, the computation runs *much* faster than before.  When we want to bring the device array back to the host memory, we can use the `copy_to_host()` method:"]},{"cell_type":"code","metadata":{"id":"mTFxHiamioVP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"3f52d050-a777-4d24-f9be-5578a109f058","executionInfo":{"status":"ok","timestamp":1580070665587,"user_tz":300,"elapsed":184,"user":{"displayName":"Geoffrey Woollard","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDjNy-VYGrpEYVchU7TMBqcYGYexvL_pBIe7j5R=s64","userId":"09068754665017449583"}}},"source":["out_host = out_device.copy_to_host()\n","print(out_host[:10])"],"execution_count":9,"outputs":[{"output_type":"stream","text":["[ 0.  3.  6.  9. 12. 15. 18. 21. 24. 27.]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vlHugNm4ioVS","colab_type":"text"},"source":["## CuPy Interoperability\n","\n","Recent versions of CuPy (>= 4.5) support (Numba's generic CUDA array interface)[https://numba.pydata.org/numba-doc/latest/cuda/cuda_array_interface.html].  We can see this on a CuPy array, by looking for the `__cuda_array_interface__` attribute:"]},{"cell_type":"code","metadata":{"id":"09pBQs6_ioVT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":102},"outputId":"f103ad73-5180-4719-fe57-8cfcbb93d9c7","executionInfo":{"status":"ok","timestamp":1580070674063,"user_tz":300,"elapsed":2337,"user":{"displayName":"Geoffrey Woollard","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDjNy-VYGrpEYVchU7TMBqcYGYexvL_pBIe7j5R=s64","userId":"09068754665017449583"}}},"source":["import cupy as cp\n","\n","x_cp = cp.asarray(x)\n","y_cp = cp.asarray(y)\n","out_cp = cp.empty_like(y_cp)\n","\n","x_cp.__cuda_array_interface__"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'data': (139785504183296, False),\n"," 'descr': [('', '<f4')],\n"," 'shape': (100000,),\n"," 'typestr': '<f4',\n"," 'version': 0}"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"1NZcWXIFioVW","colab_type":"text"},"source":["This describes the CuPy array in a portable way so that other packages, like Numba, can use it:"]},{"cell_type":"code","metadata":{"id":"TNK2FBXEioVX","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"c3c35ba3-dbb9-47f6-d449-2c91cc2e8fc3","executionInfo":{"status":"ok","timestamp":1580070675686,"user_tz":300,"elapsed":169,"user":{"displayName":"Geoffrey Woollard","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDjNy-VYGrpEYVchU7TMBqcYGYexvL_pBIe7j5R=s64","userId":"09068754665017449583"}}},"source":["add_ufunc(x_cp, y_cp, out=out_cp)\n","\n","print(out_cp[:10])"],"execution_count":11,"outputs":[{"output_type":"stream","text":["[ 0.  3.  6.  9. 12. 15. 18. 21. 24. 27.]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"X64i8B1DioVZ","colab_type":"text"},"source":["And it runs the same speed as using the Numba device allocation:"]},{"cell_type":"code","metadata":{"id":"tHTWB_CgioVa","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"cc444029-598f-47a4-a939-cbe85424af75","executionInfo":{"status":"ok","timestamp":1580070680345,"user_tz":300,"elapsed":3141,"user":{"displayName":"Geoffrey Woollard","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDjNy-VYGrpEYVchU7TMBqcYGYexvL_pBIe7j5R=s64","userId":"09068754665017449583"}}},"source":["%timeit add_ufunc(x_cp, y_cp, out=out_cp)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["1000 loops, best of 3: 691 Âµs per loop\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"sIOb6mTTioVd","colab_type":"text"},"source":["Note that Numba won't automatically create a CuPy array for the ufunc output, so if you want to ensure the ufunc result is saved in a CuPy array, be sure to pass an explicit `out` argument to the ufunc, as shown above."]}]}